{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Scrubbing and Pre-processing\n",
    "##**An example with real mortgage data**\n",
    "\n",
    "\n",
    "###Outline\n",
    "This module will focus on the mechanics of the follow data transformations and not their economic/analytic rationale (that will be covered in ____HERE___ ).\n",
    "\n",
    "In this module we scrub and pre-process raw mortgage data in preparation for regression analysis. This will involve:\n",
    "1. filling missing values;\n",
    "2. transforming raw covariates;\n",
    "3. filtered joins\n",
    "\n",
    "###Data Sources\n",
    "-  **Mortgage Origination and Performance data** is taken from the publicly available Fannie Mae Single Family Loan Performance database  http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html. We extract origiation and performance data for the 10k loans of 2005-Q1 acquisitions.\n",
    "-  **Monthly State HPI data** is taken from Freddie Mac's Home Price Index (HPI) archive http://www.freddiemac.com/finance/fmhpi/archive.html.\n",
    "-  **Monthly Mortgage Benchmark Rate data** is taken from Freddie Mac's Historical Monthly 30 year Fixed Rate index, http://www.freddiemac.com/pmms/pmms_archives.html.\n",
    "\n",
    "\n",
    "###Data Description and Roadmap\n",
    "1. Missing values will be filled with the mean value of similar mortgages that are not missing that value. For example, to impute a missing DTI (debt-to-income) ratio we bucket loans by credit score and loan interest rate and calculate their mean. We similarly bucket other characteristics to impute other missing characteristics. There is a trade-off in bucket width between specificity and idiosyncracy. We want our buckets to contain informative comparables, but not be so granular as to impute idiosyncratic or noisy values.  \n",
    "2. Covariate transformation includes a) converting a loan's state to a binary value indicating whether the property is in California or Florida, b) calculate the difference between the loan's interest rate and the prevailing Monthly Mortgage Benchmark Rate  \n",
    "3. For each mortgage we would like to:  \n",
    "  1. calculate the lowest HPI expressed as a % of starting value, for for each of three 2-year intervals from the loan's origination date, i.e the lowest HPI reached, divided by starting HPI over a) Years 1 & 2, b) Years 3 & 4, c) Years 5 & 6.  \n",
    "  2. We would also like to similarly calculate the greatest absolute reduction in the Monthly Mortgage Benchmark Rate over each of three 2-year intervals from the loan's origination.\n",
    "  3. We also want to create a response variable identifying whether the loan exited during intervals (0,2], (2,4], (4,6] and if so, when and for what reason, i.e. prepay or default.\n",
    "\n",
    "###Let's go..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our data sources, preview them, and select the columns and create our base table to which we'll add columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Orig = pd.read_csv(\"2005-1-Orig.csv\")\n",
    "Perf = pd.read_csv(\"2005-1-Perf.csv\")\n",
    "Rates = pd.read_csv(\"BenchmarkRatesto2016.csv\")\n",
    "HPI = pd.read_csv(\"HPI.csv\")\n",
    "\n",
    "Orig.head()\n",
    "Perf.head()\n",
    "Rates.head()\n",
    "HPI.head()\n",
    "\n",
    "Log_Orig = Orig[[\"LOAN_ID\",\"ORIG_AMT\",\"ORIG_RT\",\"ORIG_DTE\",\"OCLTV\",\"DTI\",\"CSCORE_B\",\"STATE\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN_ID</th>\n",
       "      <th>ORIG_AMT</th>\n",
       "      <th>ORIG_RT</th>\n",
       "      <th>OCLTV</th>\n",
       "      <th>DTI</th>\n",
       "      <th>CSCORE_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.010000e+02</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>799.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>709.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.485218e+11</td>\n",
       "      <td>175210.986267</td>\n",
       "      <td>5.770473</td>\n",
       "      <td>72.279099</td>\n",
       "      <td>34.952381</td>\n",
       "      <td>715.623413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.561726e+11</td>\n",
       "      <td>83522.447157</td>\n",
       "      <td>0.308034</td>\n",
       "      <td>16.674037</td>\n",
       "      <td>12.063991</td>\n",
       "      <td>60.735888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.001936e+11</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>473.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.296775e+11</td>\n",
       "      <td>113000.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>674.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.523753e+11</td>\n",
       "      <td>159000.000000</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>723.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.663707e+11</td>\n",
       "      <td>232000.000000</td>\n",
       "      <td>5.875000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>765.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.994329e+11</td>\n",
       "      <td>556000.000000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>821.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LOAN_ID       ORIG_AMT     ORIG_RT       OCLTV        DTI  \\\n",
       "count  8.010000e+02     801.000000  801.000000  799.000000  84.000000   \n",
       "mean   5.485218e+11  175210.986267    5.770473   72.279099  34.952381   \n",
       "std    2.561726e+11   83522.447157    0.308034   16.674037  12.063991   \n",
       "min    1.001936e+11   25000.000000    5.000000    7.000000   4.000000   \n",
       "25%    3.296775e+11  113000.000000    5.500000   65.000000  26.750000   \n",
       "50%    5.523753e+11  159000.000000    5.750000   76.000000  36.000000   \n",
       "75%    7.663707e+11  232000.000000    5.875000   80.000000  43.500000   \n",
       "max    9.994329e+11  556000.000000    6.750000  104.000000  64.000000   \n",
       "\n",
       "         CSCORE_B  \n",
       "count  709.000000  \n",
       "mean   715.623413  \n",
       "std     60.735888  \n",
       "min    473.000000  \n",
       "25%    674.000000  \n",
       "50%    723.000000  \n",
       "75%    765.000000  \n",
       "max    821.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Log_Orig[Log_Orig['OCLTV'].isnull() | Log_Orig['ORIG_RT'].isnull() | Log_Orig['CSCORE_B'].isnull() | Log_Orig['DTI'].isnull()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the first row, '**count**', we see there are 801 loans with one or more missing value. All 801 loans with missing values do have values for ORIG_RT, 2 loans (801 minus 799) are missing OCLTV, 717 loans (801 minus 84) are missing DTI, and 92 (801 minus 709) are missing CSCORE_B.\n",
    "\n",
    "###Populating Missing Data\n",
    "\n",
    "We'll show a method of bucketing the loans on-the-fly (we could instead have created additional columns indicating the bucket of each loan and run a groupby on those columns), so we pass the groupby method a function describing how to bucket each loan.\n",
    "\n",
    "Based on these groupings we run the fillna method to set missing values to the mean of the respective group.\n",
    "\n",
    "This method should catch all missing values, except where missing values are alone in their designated bucket without any other loans from which to impute a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create helper function used to bucket each loan\n",
    "def Buckets(table,x, fillVar):\n",
    "\n",
    "    if (fillVar == 'CSCORE'): # for a missing CSCORE, bucket by OCLTV and ORIG_RT\n",
    "        (OCLTV,Rate) = (round(table['OCLTV'].loc[x]/20,0)*20, round(table['ORIG_RT'].loc[x],0))\n",
    "        if np.isnan(OCLTV): OCLTV = round(table['OCLTV'].mean()/20,0)*20\n",
    "        return \"OCLTV-{}/Rate-{}\".format(OCLTV,Rate)\n",
    "    \n",
    "    elif (fillVar == 'DTI') or (fillVar == 'OCLTV'): #for a missing DTI or OCLTV, bucket by CSCORE and ORIG_RT\n",
    "        (FICO,Rate) = (round(table['CSCORE_B'].loc[x]/50,0)*50, round(table['ORIG_RT'].loc[x],0))\n",
    "        if np.isnan(FICO): FICO = round(table['CSCORE_B'].mean()/50,0)*50\n",
    "        return \"FICO-{}/Rate-{}\".format(FICO,Rate)\n",
    "\n",
    "\n",
    "# Pass the helper function to the groupby method, and tell it to fill NAs using the mean of that bucket\n",
    "fill_mean = lambda x: x.fillna(x.mean())\n",
    "\n",
    "Log_Orig['CSCORE_B'] = Log_Orig.groupby(lambda x: Buckets(Log_Orig, x, 'CSCORE'))['CSCORE_B'].apply(fill_mean)\n",
    "Log_Orig['DTI'] = Log_Orig.groupby(lambda x: Buckets(Log_Orig, x, 'DTI'))['DTI'].apply(fill_mean)\n",
    "Log_Orig['OCLTV'] = Log_Orig.groupby(lambda x: Buckets(Log_Orig, x, 'OCLTV'))['OCLTV'].apply(fill_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see what missing values remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN_ID</th>\n",
       "      <th>ORIG_AMT</th>\n",
       "      <th>ORIG_RT</th>\n",
       "      <th>ORIG_DTE</th>\n",
       "      <th>OCLTV</th>\n",
       "      <th>DTI</th>\n",
       "      <th>CSCORE_B</th>\n",
       "      <th>STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8090</th>\n",
       "      <td>830690271521</td>\n",
       "      <td>55000</td>\n",
       "      <td>6.25</td>\n",
       "      <td>01/2005</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>473</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LOAN_ID  ORIG_AMT  ORIG_RT ORIG_DTE  OCLTV  DTI  CSCORE_B STATE\n",
       "8090  830690271521     55000     6.25  01/2005     39  NaN       473    FL"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Log_Orig[Log_Orig['OCLTV'].isnull() | Log_Orig['ORIG_RT'].isnull() | Log_Orig['CSCORE_B'].isnull() | Log_Orig['DTI'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could rerun our stratification approach using slightly coarser buckets, but for simplicity we will impute the sample mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Log_Orig['DTI'] = Log_Orig['DTI'].fillna(Log_Orig['DTI'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Transforming Covariates\n",
    "\n",
    "Since we will be performing date arthimetic we must convert the relevant dates into the datetime format.\n",
    "\n",
    "Identifying loans originated in California or Florida is a simple call.\n",
    "\n",
    "Calculating the Spread-AT-Origination (SATO) for each loan requires joining each loan with the Monthly Mortgage Benchmark Rate table on the origination date, and calculating the difference between the loan rate and the prevailing Benchmark rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Log_Orig['ORIG_DTE'] = pd.to_datetime(Log_Orig['ORIG_DTE'], format='%m/%Y') #format to datetime\n",
    "\n",
    "Rates.set_index(pd.to_datetime(Rates['Date']), inplace=True)\n",
    "HPI.set_index(pd.to_datetime(HPI['Month']), inplace=True)\n",
    "\n",
    "#CA-FL identifier\n",
    "Log_Orig[\"CA-FL\"] = np.where(Log_Orig[\"STATE\"].isin([\"CA\",\"FL\"]),1,0)\n",
    "\n",
    "#SATO Calculation\n",
    "Log_Orig = Log_Orig.merge(Rates, how='left', left_on='ORIG_DTE', right_index=True)\n",
    "Log_Orig.rename(columns={'Rate':'ORIG_BENCHMK'}, inplace=True)\n",
    "\n",
    "Log_Orig['SATO'] = Log_Orig['ORIG_RT'] - Log_Orig['ORIG_BENCHMK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Filtered Joins\n",
    "\n",
    "There are 2 states for which we do not have HPI data; Puerto Rico and the Virgin Islands. Since these comprise a small portion of our pool we will simply reclassify them as Hawaii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Log_Orig.ix[Log_Orig['STATE'] == 'PR','STATE'] = 'HI'\n",
    "Log_Orig.ix[Log_Orig['STATE'] == 'VI','STATE'] = 'HI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that takes a loan's origination date and State, and computes the lowest HPI as a percent of starting HPI, given an interval duration and offset. The function also computes similar values for the Benchmark rate.\n",
    "\n",
    "We map the function to our origination series and have it populate our original table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def minVal(series,origdate,offset,duration,state):\n",
    "\n",
    "    startdate = dt.date(origdate.year+offset, origdate.month, origdate.day)\n",
    "    enddate = dt.date(origdate.year+offset+duration, origdate.month, origdate.day)\n",
    "    \n",
    "    if series=='HPI':\n",
    "        return min(HPI.ix[startdate:enddate,state]) / HPI.ix[startdate,state]\n",
    "    if series=='Rate':\n",
    "        return Rates.ix[startdate,'Rate'] - min(Rates.ix[startdate:enddate,'Rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Populate intervals with lowest HPI value\n",
    "Log_Orig['HPI-Y1&2']=map(lambda x, y: minVal('HPI',x,0,2,y), Log_Orig['ORIG_DTE'], Log_Orig['STATE'])\n",
    "Log_Orig['HPI-Y3&4']=map(lambda x, y: minVal('HPI',x,2,2,y), Log_Orig['ORIG_DTE'], Log_Orig['STATE'])\n",
    "Log_Orig['HPI-Y5&6']=map(lambda x, y: minVal('HPI',x,4,2,y), Log_Orig['ORIG_DTE'], Log_Orig['STATE'])\n",
    "\n",
    "#Populate intervals with lowest benchmark rate value\n",
    "Log_Orig['Rate-Y1&2']=map(lambda x, y: minVal('Rate',x,0,2,y), Log_Orig['ORIG_DTE'], Log_Orig['STATE'])\n",
    "Log_Orig['Rate-Y3&4']=map(lambda x, y: minVal('Rate',x,2,2,y), Log_Orig['ORIG_DTE'], Log_Orig['STATE'])\n",
    "Log_Orig['Rate-Y5&6']=map(lambda x, y: minVal('Rate',x,4,2,y), Log_Orig['ORIG_DTE'], Log_Orig['STATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Adding a Response Variable\n",
    "\n",
    "We have spent alot of effort preparing the covariate data, filling missing values, and attaching macroeconomic data. Let's now turn our attention to the response variable, which to fill we need to look at the performance data table.\n",
    "\n",
    "We write a function that returns the date and reason for a loan's exit. The __Zero Balance Code__ is Fannie Mae's way of encoding an event that causes a loan to be removed from the pool - prepayment/refinance, or a variety of different liquidation pathways like short sale or deed-in-lieu. Natural payment is also an option but since these are 30 year mortgages we do not have enough data.\n",
    "\n",
    "In addition to extracting the default date and type, we encode it into variable that tells us which if any of the first three 2-year intervals the event falls into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractDefault(ID):\n",
    "    a = Perf.ix[(Perf['LOAN_ID']==ID) & Perf['ZB_DTE'].notnull(),['Zero.Bal.Code','ZB_DTE']]\n",
    "    if len(a) == 0: return np.NAN,np.NAN\n",
    "    else: return a.iloc[0,0], a.iloc[0,1]\n",
    "    \n",
    "Log_Orig['DEFAULT.CODE'], Log_Orig['DEFAULT.DATE'] = zip(*map(extractDefault,Log_Orig['LOAN_ID']))\n",
    "\n",
    "Log_Orig['DEFAULT.DATE'] = pd.to_datetime(Log_Orig['DEFAULT.DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ResponseValue(Code,OrigDate,DefDate):\n",
    "    if Code in [3,6,9]:\n",
    "        DefDate = dt.date(DefDate.year, DefDate.month, DefDate.day)\n",
    "        \n",
    "        if DefDate <= dt.date(OrigDate.year+2, OrigDate.month, OrigDate.day):\n",
    "            return 1\n",
    "        elif DefDate <= dt.date(OrigDate.year+4, OrigDate.month, OrigDate.day):\n",
    "            return 2\n",
    "        elif DefDate <= dt.date(OrigDate.year+6, OrigDate.month, OrigDate.day):\n",
    "            return 3\n",
    "    return 0\n",
    "        \n",
    "Log_Orig['RESPONSE'] = map(ResponseValue, Log_Orig['DEFAULT.CODE'], Log_Orig['ORIG_DTE'], Log_Orig['DEFAULT.DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Conclusion\n",
    "Well that's it. We've taken raw mortgage data, cleaned it, transformed some of it, and appended relevant macroeconomic changes preparing it for use in regression analysis. Please see Part 2 as the adventure continues. There we will explore why we selected and transformed the variables we did, and interpret a logistic regression run on the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
